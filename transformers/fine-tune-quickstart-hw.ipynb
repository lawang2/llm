{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于transformers实现模型微调训练的主要流程，包括：\n",
    "- 数据集下载\n",
    "- 数据预处理\n",
    "- 训练超参数配置\n",
    "- 训练评估指标设置\n",
    "- 训练器基本介绍\n",
    "- 实战训练\n",
    "- 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import pkgs\n",
    "from datasets import load_dataset\n",
    "\n",
    "import random\n",
    "import padas as pd\n",
    "import datasets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base \n",
    "model_dir = \"models/bert-base-cased-finetune-yelp\"\n",
    "model_name_or_path = \"hugging face model file path\" # TODO\n",
    "\n",
    "## function\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column]=df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手动下载模型，使用huggingface-cli\n",
    "# huggingface-cli download --resume-download bert-base-cased --local-dir bert-base-cased\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding = \"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "show_random_elements(tokenized_datasets[\"train\"], num_examples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 数据抽样\n",
    "\n",
    "# shuffle()函数会随机重新排列列的值。如果您希望对用于洗牌数据集的算法有更多控制，可以在此函数中指定generator参数来使用不同的numpy.random.Generator。\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 微调训练配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最重要配置：模型权重保存路径（output_dir）\n",
    "\n",
    "training_args = TrainingArguments(output_dir=model_dir,\n",
    "                                  per_device_train_batch_size=16,\n",
    "                                  num_train_epochs=5,\n",
    "                                  logging_steps=100)\n",
    "\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练过程中的指标评估（Evaluate）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions,references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练过程指标监控\n",
    "training_args = TrainingArguments(output_dir=model_dir,\n",
    "                                  evaluation_strategy=\"epoch\", \n",
    "                                  per_device_train_batch_size=16,\n",
    "                                  num_train_epochs=3,\n",
    "                                  logging_steps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  train_dataset=small_train_dataset,\n",
    "                  eval_dataset=small_eval_dataset,\n",
    "                  compute_metrics=compute_metrics)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用nvidia-smi查看GPU的使用\n",
    "!watch -n 1 nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 测试\n",
    "\n",
    "small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=64).select(range(100))\n",
    "\n",
    "trainer.evaluate(small_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存模型和训练状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_dir)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework:使用完整的yelpReviewFull数据集训练，看看ACC最高到多少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Homework:使用完整的yelpReviewFull数据集训练bert-base-cased模型，看看ACC最高到多少\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions,references=labels)\n",
    "\n",
    "training_args = TrainingArguments(output_dir=model_dir,\n",
    "                                  evaluation_strategy=\"epoch\", \n",
    "                                  per_device_train_batch_size=16,\n",
    "                                  num_train_epochs=3,\n",
    "                                  # logging_steps=30 # 这个logging会不会占满磁盘，查找函数，然后限制logging的数量\n",
    "                                  )  \n",
    "\n",
    "full_train_dataset = tokenized_datasets[\"train\"]\n",
    "full_eval_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  train_dataset=full_train_dataset,\n",
    "                  eval_dataset=full_eval_dataset,\n",
    "                  compute_metrics=compute_metrics)\n",
    "\n",
    "trainer.train() ## 查看 training loss 和 validation loss 和 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 测试\n",
    "\n",
    "full_test_dataset = tokenized_datasets[\"test\"]\n",
    "trainer.evaluate(full_test_dataset) ## 查看 test loss 和 accuracy"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
